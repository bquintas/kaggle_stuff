{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. <a href=\"#1\">Set up AutoGluon</a>\n",
    "2. <a href=\"#2\">Read the datasets </a>\n",
    "3. <a href=\"#3\">Train a Classifier with AutoGluon</a>\n",
    "4. <a href=\"#4\">Classifier evaluation</a>\n",
    "5. <a href=\"#5\">Clean up model artifacts</a>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. <a name=\"1\">Set up AutoGluon</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Let's install Autogluon. This may take some time as it installs all required libraries for AutoGluon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pip==21.3.1\n",
    "! pip install setuptools==54.1.1\n",
    "! pip install wheel==0.36.2\n",
    "! pip install mxnet==1.7.0.post2\n",
    "! pip install autogluon==0.1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. <a name=\"2\">Read the datasets</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Let's read the training and test datasets into dataframes, using Pandas. (AutoGluon will handle the validation itself)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "  \n",
    "training_data = pd.read_csv('../../data/titanic/train.csv')\n",
    "test_data = pd.read_csv('../../data/titanic/test.csv')\n",
    "\n",
    "print('The shape of the training dataset is:', training_data.shape)\n",
    "print('The shape of the test dataset is:', test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__AutoGluon__ will handle the __validation data__ itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. <a name=\"3\">Train a Classifier with AutoGluon</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "We can run AutoGluon with a short snippet. For fitting, we just call the __.fit()__ function. In this exercise, we used the data frame objects, but this tool also accepts the raw csv files as input. To use this tool with simple csv files, you can follow the code snippet below.\n",
    "\n",
    "```python\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "\n",
    "train_data = TabularDataset(file_path='path_to_dataset/train.csv')\n",
    "test_data = TabularDataset(file_path='path_to_dataset/test.csv')\n",
    "\n",
    "predictor = TabularPredictor(label='label_column').fit(train_data)\n",
    "test_predictions = predictor.predict(test_data)\n",
    "```\n",
    "\n",
    "We have our separate __data frames__ for training and test data, so we work with them below. We grab all the data points. You can also pass the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "\n",
    "\n",
    "k = training_data.shape[0] # grab the whole dataset\n",
    "\n",
    "predictor = TabularPredictor(label='Survived').fit(training_data.head(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also summarize what happened during fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.fit_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. <a name=\"4\">Prediction and Evaluation</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Next, load separate test data to demonstrate how to make predictions on new examples at inference time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run predictions for the test dataset\n",
    "test_predictions = predictor.predict(test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the performance of each individual trained model on the test data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. <a name=\"5\">Write predictions</a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "result_df = pd.DataFrame(columns=[\"PassengerId\", \"Survived\"])\n",
    "result_df[\"PassengerId\"] = test_data[\"PassengerId\"].tolist()\n",
    "result_df[\"Survived\"] = test_predictions\n",
    "\n",
    "result_df.to_csv(\"../../data/titanic/titanic_survived.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Double-check submission file against the gender_submission.csv')\n",
    "sample_submission_df = pd.read_csv('../../data/titanic/gender_submission.csv')\n",
    "print('Differences between test data PassengerId and sample submission IDs:',(sample_submission_df['PassengerId'] != result_df['PassengerId']).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
